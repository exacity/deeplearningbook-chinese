% Translator: Shenjian Zhao
\chapter{前言}
\label{chap:introduction}
自古以来，创造者就梦想着创造能思考的机器。
这个愿望至少可以追溯到古希腊的时期。
神话人物皮格马利翁(Pygmalion)、代达罗斯(Daedalus)和赫淮斯托斯(Hephaestus)都可以被视为传说中的发明家，而加拉蒂亚(Galatea)、塔洛斯(Talos)和潘多拉(Pandora)都可以被看作是人造生命\citep{ovid2004metamorphoses,sparkes1996red,1997works}。

当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能，尽管这距造出一台还有一百年多年之久\citep{Lovelace1842}。
如今，\firstacr{AI}是一个具有许多实际应用和活跃研究课题的领域，并蓬勃发展着。
我们指望通过智能软件自动化处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。

在\gls{AI}的早期，那些对人类智力来说是困难但对计算机来说是相对简单的问题得到迅速解决，比如那些可以通过一系列形式的数学规则来描述的问题。
\gls{AI}的真正挑战被证明是解决对人来说很容易执行，但很难形式化描述的任务，也就是我们人类能自动的靠直观解决的问题，比如识别说的话或图像中的脸。

这本书讨论这些更直观的问题一种解决方案。
这种解决方案是为了让计算机从经验中学习，并通过层次化概念体系来理解世界，其中每个概念通过与较简单概念之间的联系来定义。
让计算机通过经验获取知识可以避免人工形式地指定的计算机需要的所有知识。
层次化的概念让计算机通过构建较简单的概念来学习复杂概念。
如果绘制出这些概念如何建立在彼此之上的图，我们将得到一张``深''（层次很多）的图。
出于这个原因，我们称这种方法为\glssymbol{AI}\firstgls{DL}。

% -- 1 --

许多\glssymbol{AI}的早期成功发生在相对干净且正式的环境中， 计算机不需要具备很多关于世界的知识。
例如，IBM的深蓝（Deep Blue）国际象棋系统在1997年击败了世界冠军\ENNAME{Garry Kasparov}\citep{Hsu2002}。
当然国际象棋是一个非常简单的领域，仅含有64个位置并只能以严格限制的方式移动32个棋子。
设计一种成功的国际象棋策略是一个巨大的成就，但挑战并不是向计算机描述棋子和允许的移动的困难性。
国际象棋完全可以由一个非常简短的、完全形式化的规则列表描述，并可以轻松由程序员提前提供。

讽刺的是，抽象和形式的任务对人类而言是最困难的脑力任务之一，对计算机而言却属于最容易的。
计算机早已能够打败即便是最好的人类棋手，但直到最近才在识别对象或语音的任务中到达匹配人类平均的能力。
一个人的日常生活需要关于世界的巨量知识。
很多这方面的知识是主观的、直观的，因此很难通过形式的方式表达清楚。
为了表现出智能，计算机需要获取同样的知识。
\gls{AI}的一个关键挑战就是如何将这些非形式的知识传达给计算机。

一些\gls{AI}项目都力求将关于世界的知识用形式化的语言进行硬编码。
计算机可以通过这些形式化语言自动地使用逻辑推理规则来理解声明。
这就是所谓的\gls{AI}的\firstgls{knowledge_base}方法。
这些项目都没有导致重大的成功。
其中最著名的项目是的Cyc \citep{Lenat-1989-book}。
Cyc包括一个\gls{inference}引擎和一个使用CycL语言描述的声明数据库。
这些声明是由人类监督者输入的。
这是一个笨拙的过程。
人们设法设计出具有足够复杂性，能准确描述世界的形式规则。
例如，Cyc不能理解一个关于名为\ENNAME{Fred}的人在早上剃须的故事\citep{MachineChangedWorld}。
它的推理引擎检测到故事中的不一致性：它知道人没有电气零件，但由于\ENNAME{Fred}拿着一个电动剃须刀，它认为实体``FredWhileShaving''含有电气部件。
因此就会产生这样的疑问——\ENNAME{Fred}在刮胡子的时候是否仍然是一个人。

依靠硬编码的知识体系面对的困难表明，\glssymbol{AI}系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。
这种能力被称为\firstgls{ML}。
引入\gls{ML}使计算机能够解决涉及现实世界知识的问题，并能作出看似主观的决策。
所谓\firstgls{logistic_regression}的简单\gls{ML}算法可以决定是否建议剖腹产\citep{MorYosef90}。
所谓\firstgls{naive_bayes}的简单\gls{ML}算法可以从垃圾邮件中区分合法的电子邮件。

% -- 2 --

这些简单的\gls{ML}算法的性能在很大程度上依赖于给定数据的\firstgls{representation}。
例如，当\gls{logistic_regression}被用于推荐剖腹产时，\glssymbol{AI}系统不直接检查患者。
相反，需要医生告诉系统几条相关的信息，诸如子宫疤痕是否存在。
表示患者的每条信息被称为一个特征。
\gls{logistic_regression}学习病人的这些特征如何与各种结果相关联。
然而，它丝毫不能影响该特征定义的方式。
如果将病人的MRI扫描作为\gls{logistic_regression}的输入，而不是医生正式的报告，它将无法作出有用的预测。
MRI扫描的单一像素与分娩过程中的并发症只有微不足道的相关性。

对\gls{representation}的依赖是在整个计算机科学乃至日常生活中出现的普遍现象。
在计算机科学中，如果数据集合已经过明智地结构化并建立索引，数据操作的处理速度可以成倍的加快（如搜索）。
人们可以很容易地在阿拉伯数字的表示下进行算术运算，但在罗马数字的表示下运算会更耗时。
毫不奇怪，表示的选择会对机器学习算法的性能产生巨大的影响。
图\ref{fig:chap1_polar}显示了一个简单的可视化例子。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/polar_color}}
\fi
\caption{TODO}
\label{fig:chap1_polar}
\end{figure}

许多\gls{AI}的任务都可以通过提取一个合适的特征集，然后将这些特征提供给简单的\gls{ML}算法来解决。
例如，从声音鉴别说话者的一个有用特征是说话者声道大小的估计。
这个特征为判断说话者是一个男性，女性还是儿童提供了有力线索。

然而，对于许多任务来说，很难知道应该提取哪些特征。
例如，假设我们想编写一个程序来检测照片中的车。
我们知道，汽车有轮子，所以我们可能会想用车轮的存在作为特征。
不幸的是，我们难以准确地从像素值的角度描述一个车轮看起来如何。
车轮具有简单的几何形状，但它的图像可以因为环境而变得很复杂，如落在车轮上的阴影、太阳照亮的车轮的金属零件、汽车的挡泥板或者遮挡的车轮一部分的前景物体，等等。

% -- 3 --

解决这个问题一个途径的是使用\gls{ML}来发现\gls{representation}本身，而不仅仅把表示映射到输出。
这种方法被称为\firstgls{representation_learning}。
学习到的表示往往获得比手动设计的表示更好的性能。
并且它们只需最少的人工干预, 就能让\glssymbol{AI}系统迅速适应新的任务。
\gls{representation_learning}算法只需几分钟就可以为简单的任务发现一个很好的特征集，对于复杂任务则需要几小时到几个月。
手动为一个复杂的任务设计特征需要耗费大量的人工时间和精力；甚至需要花费整个社群研究人员几十年时间。

一个\gls{representation_learning}算法的典型例子是\firstgls{AE}。
\gls{AE}是组合了将输入转换到不同表示\firstgls{encoder}函数和将新的表示转回原来形式的\firstgls{decoder}函数。 
\gls{AE}的训练目标是，输入经过\gls{encoder}和\gls{decoder}之后尽可能多的保留信息，同时希望新的\gls{representation}有各种好的属性。不同种类的\gls{AE}的目标是实现不同种类的属性。

当设计特征或学习特征的算法时，我们的目标通常是分离出能解释观察数据的\textbf{变化因素}(factors of variation)。
在此背景下，``因素''这个词仅指代的影响不同来源；因素通常不是乘性组合。
这些因素通常是不能被直接观察到的量。
相反，他们可能是现实世界中观察不到的物体或者不可观测的力，但影响能观测到的量。
他们还存在于人类的思维构造中，为了给观察到的数据提供有用的简化解释或推断原因。
它们可以被看作帮助我们了解富有变化的数据的概念或者抽象。
当分析语音记录时，变化的因素包括说话者的年龄、性别、他们的口音和他们正在说的词语。
当分析汽车的图像时，变化的因素包括汽车的位置、它的颜色、太阳的角度和亮度。

% -- 4 --

在许多现实世界的\gls{AI}应用困难的一个重要原因是很多因素的变化影响着我们能够观察到的每一个数据。
在夜间，一张图片中红色汽车的单个像素可能是非常接近黑色。
汽车轮廓的形状取决于视角。
大多数应用程序需要我们\emph{理清}变化的因素并丢弃我们不关心的因素。

当然，从原始数据中提取这样的高层次、抽象的特征是非常困难的。
许多这样的变化因素，诸如说话者的口音，只能对数据进行复杂的、接近人类水平的理解来确定。
它几乎与获得原来问题的表示一样困难，乍一看，\gls{representation_learning}似乎并不能帮助我们。

\firstgls{DL}通过其它较简单的表示来表达复杂表示，解决了\gls{representation_learning}中核心问题。

\gls{DL}让计算机通过较简单概念构建复杂的概念。
图\ref{fig:chap1_deep_learning}显示了\gls{DL}系统通过组合较简单的概念，例如转角和轮廓，转而定义边缘来表示图像中一个人的概念。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/deep_learning}}
\fi
\caption{TODO}
\label{fig:chap1_deep_learning}
\end{figure}
\gls{DL}模型的典型例子是前馈深度网络或\firstacr{MLP}。
\gls{MLP}仅仅是一个将一组输入值映射到输出的数学函数。
该函数由许多较简单的函数组合而构成。
我们可以认为每个应用具有不同的数学函数，并为输入提供新的表示。

学习数据的正确表示的想法是解释\gls{DL}的一个观点。
另一个观点是深度允许计算机学习一个多步骤的计算机程序。
表示的每一层可以被认为是并行执行另一组指令后计算机的存储器状态。
更深的网络可以按顺序执行更多的指令。
顺序指令提供了极大的能力，因为后面的指令可以参考早期指令的结果。
根据\gls{DL}这个观点，一层的激活函数没有必要对解释输入变化的因素进行编码。
\gls{representation}还存储着协助程序执行的状态信息，使输入更加有意义。
这里的状态信息是类似于传统计算机程序中的计数器或指针。
它与具体的输入内容无关，但有助于模型组织其处理过程。

% -- 6 --

主要有两种测量模型深度的方式。
第一观点是基于评估架构所需执行的顺序指令的数目。
我们可以认为这是描述每个给定输入后，计算模型输出的流程图的最长路径。
正如两个等价的计算机程序根据不同的语言将具有不同的长度，相同的函数可以被绘制为具有不同深度的流程图，这取决于我们允许使用的单一步骤的函数。
图\ref{fig:chap1_language}说明了语言的选择怎样给相同的架构两个不同的衡量。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/language}}
\fi
\caption{TODO}
\label{fig:chap1_language}
\end{figure}

另一种是在深度概率模型中使用的方法，不是将计算图的深度视为模型深度，而是将描述概念如何彼此相关的图的深度视为模型深度。
在这种情况下，计算每个概念表示的计算流程图的深度可能比概念本身的图更深。
这是因为系统对较简单概念的理解可以在给出更复杂概念的信息后进一步细化。
例如，一个\glssymbol{AI}系统观察一张其中一只眼睛在阴影中的脸部图像,最初可能只看到一只眼睛。
当检测到的脸部的存在后，它可以推断的第二只眼睛也可能是存在的。
在这种情况下，概念的图仅包括两层——关于眼睛的层和关于脸的层,但是，如果我们根据每个概念给出的其它$n$次估计进行改进，计算的图将包括$2n$个层。

% -- 7 --

由于并不总是清楚计算图的深度或概率模型图的深度哪一个是最相关的，并且由于不同的人选择不同的最小元素集来构建相应的图，导致架构的深度不存在单一的正确值，就像计算机程序的长度不存在单一的正确值。
也不存在模型多少深才能被修饰为``深''的共识。
<BAD>但是，\gls{DL}可以可靠地被视为比传统机器学习涉及更大量的学到函数或学到概念组合的模型的研究。

总之， 这本书的主题——\gls{DL}是\glssymbol{AI}的途径之一。
具体地讲，它是\gls{ML}的一种，一种允许计算机系统能从经验和数据中得到提高的技术。

我们主张\gls{ML}是构建能在复杂实际环境下运行的\glssymbol{AI}系统的唯一可行方法。
\gls{DL}是一种特定类型的\gls{ML}，通过将世界表示为由较简单概念定义复杂概念，从一般抽象到高级抽象的嵌套概念体系获得极大的能力和灵活性。
图\ref{fig:chap1_venn}说明了这些不同的\glssymbol{AI}学科之间的关系。图\ref{fig:chap1_which_part_learned}给出了每个学科如何工作的一个高层次的原理。
\begin{figure}[!hbt]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/venn}}
\fi
\caption{TODO}
\label{fig:chap1_venn}
\end{figure}
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/which_part_learned}}
\fi
\caption{TODO}
\label{fig:chap1_which_part_learned}
\end{figure}

\section{谁应该读这本书？}
\label{sec:who_should_read_this_book}

这本书对各类读者都一定用处的，但我们是基于两个主要目标受众而写的。
其中一个目标受众是学习\gls{ML}的大学生（本科或研究生），包括那些开始了职业生涯的\gls{DL}和\gls{AI}研究者。
另一个目标群体是没有\gls{ML}或统计背景但要迅速在他们的产品或平台开始使用\gls{DL}的软件工程师。
\gls{DL}在许多软件领域都已被证明是有用的，包括计算机视觉、语音和音频处理、自然语言处理、机器人技术、生物信息学和化学、电子游戏、搜索引擎、网络广告和金融。

% -- 8 --

为了最好地适应各类读者，这本书被组织为三个部分。
第一部分介绍了基本的数学工具和\gls{ML}的概念。
<BAD>第二部分介绍了本质上已解决的技术、最成熟的\gls{DL}算法。
第三部分介绍了被广泛认为是深度学习未来研究重点的但更具猜测性的想法。

读者可以随意跳过不感兴趣或与自己背景不相关的部分。
熟悉线性代数、概率和基本\gls{ML}概念的读者可以跳过第一部分，例如，当读者只是想实现一个能工作的系统则不需要阅读超出第二部分的内容。
为了帮助读者选择章节，图\ref{fig:chap1_dependency}展示了这本书的高层组织结构的流程图。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/dependency}}
\fi
\caption{TODO}
\label{fig:chap1_dependency}
\end{figure}

% -- 10 --

我们假设所有读者都具备计算机科学背景。
也假设读者熟悉编程，并且对计算的性能问题、复杂性理论、入门级微积分和一些图论的术语有基本的了解。

\section{深度学习的历史趋势}
\label{sec:historical_trends_in_deep_learning}
通过历史背景了解\gls{DL}是最简单的方式。
我们指出了\gls{DL}的几个关键趋势，而不是提供详细的历史：
\begin{itemize}
 \item \gls{DL}有着悠久而丰富的历史，但伴随着很多反映不同哲学观点名称的尘封而渐渐消逝。
 \item 可用的训练数据的量已经增加，使得\gls{DL}变得更加有用。
 \item 随着时间的推移，针对\gls{DL}的计算机软硬件基础设施都有所改善，\gls{DL}模型的规模也随之增长。
 \item \gls{DL}已经解决日益复杂的应用，并随着时间的推移,精度不断提高。
\end{itemize}

\subsection{神经网络的众多名称和命运变迁}
\label{sec:the_many_names_and_changing_fortunes_of_neural_networks}

我们期待这本书的许多读者都听说过\gls{DL}这一激动人心的新技术，并为一本书提及关于一个新兴领域的``历史''而感到惊讶。
事实上，\gls{DL}的历史可以追溯到20世纪40年代。
\gls{DL}只是\emph{看上去像}一个新的领域，因为在目前流行的前几年它是相对冷门的，同时也因为它被赋予了许多不同的已经消逝的名称，最近才成为所谓的``深度学习''。
这个邻域已经更换了很多名称，反映了不同的研究人员和不同观点的影响。

讲述整个综合性的\gls{DL}历史超出了本书的范围。
然而，一些基本的背景对理解\gls{DL}是有用的。
一般来说，目前为止已经有三次\gls{DL}的发展浪潮：在20世纪40年代到60年代\gls{DL}被称为\firstgls{cybernetics}，20世纪80年代到90年代\gls{DL}被誉为\firstgls{connectionism}，并于2006年开始，\gls{DL}在当前的名称下开始复苏。
这在图\ref{fig:chap1_cybernetics_connectionism_ngrams_color}中定量给出。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/cybernetics_connectionism_ngrams_color}}
\fi
\caption{TODO}
\label{fig:chap1_cybernetics_connectionism_ngrams_color}
\end{figure}

% -- 12 --

我们现在认识的一些最早的学习算法，旨在模拟生物学习的计算模型，即大脑怎样学习或为什么能学习的模型。
其结果是，已经消逝的\gls{DL}的名称之一——\firstacr{ANN}。
此时\gls{DL}模型对应的观点是他们设计的系统是受生物大脑（无论人类大脑或其他动物的大脑）所启发。
尽管有些\gls{ML}的\gls{NN}有时被用来理解大脑功能\citep{hinton1991lesioning}，它们一般都没有被设计成生物功能的真实模型。
\gls{DL}的神经观点受两个主要思想启发的。
一个想法是，大脑这个例子证明智能行为的可能性，因此建立智能概念上的直接途径是逆向大脑背后的计算原，并复制其功能。
另一种看法是，理解大脑和人类智力背后的原则将是非常有趣的，因此\gls{ML}模型除了解决工程应用的能力， 如果能阐明这些基本的科学问题也将会很有用。 

% -- 13 --
  
现代术语``\gls{DL}''超越了目前\gls{ML}模型的神经科学观点。
学习的\emph{多层次组合}这一更普遍的原则更加吸引人，这可以应用于\gls{ML}框架且不必是受神经启发的。
 
 
现代\gls{DL}的最早前身是从神经科学的角度出发的简单线性模型。
这些模型被设计为使用一组$\Sn$个输入$\Sx_1, \dots ,\Sx_n$并将它们与一个输出$\Sy$相关联。 
这些模型将学习一组权重$\Sw_1, \dots, \Sw_n $ 并计算它们的输出$f(\Vx, \Vw) = \Sx_1 \Sw_1 + \dots + \Sx_n \Sw_n$。
这第一波\gls{NN}研究的浪潮被称为\gls{cybernetics}，如图\ref{fig:chap1_cybernetics_connectionism_ngrams_color}所示。

\ENNAME{McCulloch-Pitts}神经元\citep{McCulloch43}是脑功能的早期模型。
该线性模型通过测试函数$f(\Vx，\Vw)$的是正还是负来识别两种不同类型的输入。
当然，为了使模型对应于期望的类别定义，需要正确地设置权重。
这些权重可以由操作人员设定。
在20世纪50年代，感知机\citep{Rosenblatt-1956,Rosenblatt-1958}成为第一个能根据了每个类别输入的\gls{example}来学习权重的模型。
约在同一时期，\textbf{自适应线性元件}（ADALINE），简单地返回函数$f(\Vx)$本身的值来预测一个实数\citep{Widrow60}，并且还可以学习从数据预测这些数。

这些简单的学习算法大大影响了\gls{ML}的现代景象。
用于调节ADALINE的权重的训练算法是称为\firstgls{SGD}的一种特例。
稍加修改的\gls{SGD}算法仍然是当今\gls{DL}的主要训练算法。

基于感知机和ADALINE中使用的函数$f(\Vx, \Vw)$的模型被称为\firstgls{linear_model}。
这些模型仍然是最广泛使用的\gls{ML}模型，尽管在许多情况下，它们以不同于原始模型的方式进行\emph{训练}。

\gls{linear_model}有很多局限性。
最著名的是，它们无法学习XOR函数，即$f([0,1], \Vw) = 1, f([1,0], \Vw)=1$， 
但是$f([1,1], \Vw)=0, f([0,0],\Vw)= 0$。
在\gls{linear_model}中观察到这些缺陷的评论家开始反对受生物学启发的学习\citep{Minsky69}。
这是\gls{NN}第一次热度较多的下降。

现在，神经科学被视为\gls{DL}研究的一个重要灵感来源，但它已不再是该领域的主要导向。

% -- 14 --

如今神经科学在\gls{DL}研究中的作用被削弱，主要的原因只是我们根本没有足够的关于大脑信息来作为指导。
要获得对大脑实际使用算法的深刻理解，我们需要有能力同时监测（至少是）数千相连神经元的活动。
我们不能够做到这一点，甚至连大脑的最简单、最深入研究的部分我们都还远远没有理解\citep{olshausen:2005}。

神经科学已经给了我们依靠单一\gls{DL}算法解决许多不同任务的理由。
神经学家们发现，如果将雪貂的大脑重新连接，使视觉信号传送到听觉区域，它们可以学会用大脑的听觉处理区域``看''\citep{von2000visual}。
这表明，多数哺乳动物大脑的可能使用单一的算法解决大部分大脑可以解决的不同任务。
这个假设之前，\gls{ML}研究更加分散，研究人员在不同的社区研究自然语言处理、计算机视觉、运动规划和语音识别。
如今，这些应用的社区仍然是独立的，但是\gls{DL}研究小组同时研究许多或甚至所有这些应用领域是很常见的。

我们能够从神经科学得到一些粗略的指南。
仅通过计算单元之间的相互作用而变得智能的基本思想是受大脑启发的。
新认知机\citep{Fukushima80}受哺乳动物视觉系统的结构启发，引入了一个处理图片的强大模型架构，后来成为了现代卷积网络的基础\citep{LeCun98-small}，我们将会在\ref{sec:the_neuroscientific_basis_for_convolutional_networks}节看到。
目前大多数\gls{NN}基于称为\firstgls{ReLU}的神经单元模型。
原始认知机\citep{Fukushima75}受我们关于大脑功能知识的启发， 引入了一个更复杂的版本。
简化的现代版基于许多观点进化发展，\citet{Nair-2010}和\citet{Glorot+al-AI-2011-small}援引神经科学作为影响，\citet{Jarrett-ICCV2009}援引更多面向工程的影响。
虽然神经科学是灵感的重要来源，它不需要被视为刚性指导。
我们知道，实际的神经元与现代\gls{ReLU}计算着非常不同函数，但更接近真实神经网络的系统并没有导致\gls{ML}性能的提升。
此外，虽然神经科学已经成功地启发了一些\gls{NN}\emph{架构}，但我们还没有足够的了解生物学习的神经科学，因此在训练这些架构时，不能提供给我们很多关于\emph{学习算法}的指导。

媒体报道经常强调\gls{DL}与大脑的相似性。
虽然\gls{DL}研究人员更可能比其他\gls{ML}领域（如核机器或贝叶斯统计工作的研究人员）引用大脑作为影响，人们不应该认为\gls{DL}是对模拟大脑的尝试。
现代\gls{DL}从许多领域获取灵感，特别是应用数学的基本内容如线性代数、概率论、信息论和数值优化。
虽然一些\gls{DL}的研究人员引用神经科学作为灵感的重要来源，但其他学者完全不关心神经科学。

% -- 15 --

值得注意的是，了解大脑是如何在算法层面上工作的尝试是鲜活且发展良好的。
这项尝试主要是被称为``计算神经科学''，并且是独立于\gls{DL}一个领域。
研究人员两个领域之间反复研究是很常见的。
\gls{DL}领域主要是关注如何构建智能的计算机系统，用来解决需要智能才能解决的任务，而计算神经科学领域主要是关注构建大脑如何工作的更精确的模型。

在20世纪80年代，神经网络研究的第二次浪潮在很大程度上是伴随一个被称为\firstgls{connectionism}或\textbf{并行分布处理}运动而出现的\citep{Rumelhart86,mcclelland1995appeal}。
\gls{connectionism}是在认知科学的背景下出现的。
认知科学是理解心智，并结合多个不同层次分析的跨学科方法。
在20世纪80年代初期，大多数认知科学家研究符号推理的模型。
尽管这很流行，符号模型很难解释大脑如何真正使用神经元实现推理功能。 
连接主义者开始研究实际能基于神经实现的认知模型\citep{Touretzky1985}，其中很多复苏的想法可以追溯到心理学家\ENNAME{Donald Hebb}在20世纪40年代的工作\citep{Hebb49}。

\gls{connectionism}的中心思想是，当网络将大量简单计算单元连接在一起时可以实现智能行为。
这种见解同样适用于与计算模型中隐层单元类似作用的生物神经系统中的神经元。  

上世纪80年代的\gls{connectionism}运动过程中形成的几个关键概念在今天的\gls{DL}中仍然是非常重要的。

其中一个概念是\firstgls{distributed_representation}\citep{Hinton-et-al-PDP1986}。
这一想法是，系统每个的输入应该由许多特征表示的，并且每个特征应参与许多可能输入的表示。
例如，假设我们有一个能够识别红色、绿色、或蓝色的汽车、卡车和鸟类的视觉系统。
表示这些输入的其中一个方法是将九个可能的组合：红卡车，红汽车，红鸟，绿卡车等等使用单独的神经元或\gls{hidden_unit}激活。
这需要九个不同的神经元，并且每个神经必须独立地学习颜色和对象身份的概念。
改善这种情况的方法之一是使用\gls{distributed_representation}，即用三个神经元描述颜色,三个神经元描述对象身份。 
这仅仅需要6个神经元而不是9个，并且描述红色的神经元能够从汽车、卡车和鸟类的图像中学习红色，而不仅仅是从一个特定类别的图像中学习。 
\gls{distributed_representation}的概念是本书的核心，我们将在\ref{chap:representation_learning}章中更加详细地描述。

% -- 16 --

\gls{connectionism}运动的另一个重要成就是反向传播算法的成功运用（训练具有内部表示的深度\gls{NN}）和普及\citep{RHW,Lecun-these87}。
这个算法虽然已经黯然失色不再流行，但截至写书之时，仍是训练深度模型的主要方法。

在20世纪90年代，研究人员在使用\gls{NN}进行序列建模的方面取得了重要进展。
\citet{Hochreiter91}和\citet{Bengio1994ITNN}指出了建模长序列的一些根本数学难题，将在\ref{sec:the_challenge_of_longterm_dependencies}节中描述。
\citet{Hochreiter+Schmidhuber-1997}引入\gls{LSTM}(\glssymbol{LSTM})网络来解决这些难题。
如今，\glssymbol{LSTM}在许多序列建模任务中广泛应用，包括Google的许多自然语言处理任务。

\gls{NN}研究的第二次浪潮一直持续到上世纪90年代中期。
基于\gls{NN}等其他\glssymbol{AI}技术的企业开始在寻求投资的同时，做不切实际野心勃勃的主张。
当\glssymbol{AI}研究不能实现这些不合理的期望时，投资者感到失望。
同时，\gls{ML}的其他领域取得进步。
核学习机\citep{Boser92,Cortes95,SchBurSmo99}和图模型\citep{Jordan98}都在很多重要任务上实现了很好的效果。
这两个因素导致了\gls{NN}热度的第二次下降，一直持续到2007年。

在此期间，\gls{NN}持续在某些任务上获得令人印象深刻的表现\citep{LeCun98-small,Bengio-nnlm2001}。
加拿大高级研究所（CIFAR）通过其神经计算和自适应感知（NCAP）研究计划帮助维持\gls{NN}研究。
这个计划统一了由分别由\ENNAME{Geoffrey Hinton}，\ENNAME{Yoshua Bengio}和\ENNAME{Yann LeCun}引导的多伦多大学、蒙特利尔大学和纽约大学的\gls{ML}研究小组。
CIFAR NCAP研究计划具有多学科的性质，其中还包括在人类神经科学家和计算机视觉专家。

% -- 17 --

在那个时间点上，普遍认为深度网络被难以训练的。
现在我们知道，从20世纪80年代就存在的算法能工作得非常好，但是直到在2006年前后都没有体现出来。
这个问题可能是单纯的因为计算复杂性太高，而以当时可用的硬件难以进行足够的实验。

神经网络研究的第三次浪潮开始于2006年的突破。
\ENNAME{Geoffrey Hinton}表明名为\gls{DBN}的\gls{NN}可以使用一种称为贪婪逐层训练的策略进行有效地训练\citep{Hinton06}，我们将在\ref{sec:greedy_layerwise_unsupervised_pretraining}中更详细地描述。
CIFAR附属的其他研究小组很快表明，同样的策略可以被用来训练许多其他类型的深度网络\citep{Bengio+Lecun-chapter2007-small,ranzato-07}，并能系统地帮助提高在测试样例上的泛化能力。
\gls{NN}研究的这一次浪潮普及了``\gls{DL}''这一术语的使用，强调研究人员现在可以训练以前不可能训练的更深的神经网络，并把注意力集中于深度的理论意义\citep{Bengio+Lecun-chapter2007,Delalleau+Bengio-2011-small,Pascanu-et-al-ICLR2014,Montufar-et-al-NIPS2014}。
此时，深度\gls{NN}已经优于与之竞争的基于其他\gls{ML}技术以及手工设计函数的\glssymbol{AI}系统。
神经网络流行的第三次浪潮在写这本书的时候还在继续，尽管深度学习的研究重点在这一段时间内发生了巨大变化。
第三次浪潮开始把重点放在新的无监督学习技术和深度模型从小数据集进行推广的能力，但现在更多的注意点是在更古老的监督学习算法和深度模型充分利用大型标注数据集的能力。

\subsection{与日俱增的数据量}
\label{sec:increasing_dataset_sizes}
人们可能想问，尽管人工\gls{NN}的第一个实验在20世纪50年代就完成了，为什么\gls{DL}直到最近才被认为是关键技术。
自20世纪90年代以来，\gls{DL}就已经成功用于商业应用，但通常被视为是一种艺术而不是一种技术，且只有专家可以使用的艺术，这种观点持续到最近。
确实，要从一个\gls{DL}算法获得良好的性能需要一些技巧。
幸运的是，随着训练数据的增加，所需的技巧正在减少。
目前在复杂的任务达到与人类表现的学习算法，与20世纪80年代努力解决的玩具问题(toy problem)的学习算法几乎是一样的，尽管这些算法训练的模型经历了变革，简化了极深架构的训练。
最重要的新进展是现在我们有了这些算法成功训练所需的资源。
图\ref{fig:chap1_dataset_size_color}展示了基准数据集的大小如何随着时间的推移显着增加。
这种趋势是由社会日益数字化驱动的。
由于我们的活动越来越多发生在计算机上，我们做什么也越来越多地被记录。
我们的计算机越来越多地联网在一起，变得更容易集中管理这些记录，并将它们整理成适于\gls{ML}应用的数据集。
因为``大数据''的时代\gls{ML}要容易得多，因为统计估计的主要负担——观察少量数据以在新数据上泛化——已经减轻。
截至2016年，一个粗略的经验法则是，监督\gls{DL}算法一般在每类给定约5000标注样本情况下可以实现可接受的性能，当至少有1000万标注样本的数据集用于训练时将达到或超过人类表现。
<BAD>在更小的数据集上成功工作是一个重要的研究领域，我们应特别侧重于如何通过无监督或半监督学习充分利用大量的未标注样本。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/dataset_size_color}}
\fi
\caption{TODO}
\label{fig:chap1_dataset_size_color}
\end{figure}
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics[width=0.8\textwidth]{Chapter1/figures/mnist}}
\fi
\caption{TODO}
\label{fig:chap1_mnist}
\end{figure}

% -- 20 --

\subsection{与日俱增的模型规模}
\label{sec:increasing_model_sizes}

相对20世纪80年代较少的成功，现在\gls{NN}非常成功的另一个重要原因是现在我们拥有的计算资源可以运行更大的模型。
\gls{connectionism}的主要见解之一是，当动物的许多神经元一起工作时会变得聪明。
单独神经元或小集合的神经元不是特别有用。

生物神经元没有连接的特别密集。
如图\ref{fig:chap1_number_of_synapses_color}所示，几十年来，我们的\gls{ML}模型中每个神经元的连接数量甚至与哺乳动物大脑的在同一数量级。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/number_of_synapses_color}}
\fi
\caption{TODO}
\label{fig:chap1_number_of_synapses_color}
\end{figure}

如图\ref{fig:chap1_number_of_neurons_color}所示，就神经元的总数目而言，直到最近\gls{NN}都是惊人的少。
自从引入\gls{hidden_unit}以来，人工\gls{NN}的规模大约每2.4年扩大一倍。
这种增长是由更大内存、更快的计算机和更大的可用数据集驱动的。
较大的网络能够在更复杂的任务中实现更高的精度。
这种趋势看起来将持续数十年。
除非有允许迅速扩展的新技术，否则至少直到21世纪50年代，人工\gls{NN}将才能具备与人脑相同数量级的神经元。
生物神经元表示的函数可能比目前的人工神经元更复杂，因此生物神经网络可能比图中描绘的更大。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/number_of_neurons_color}}
\fi
\caption{TODO}
\label{fig:chap1_number_of_neurons_color}
\end{figure}


现在回想起来，比一个水蛭的神经元还少的\gls{NN}不能解决复杂的\gls{AI}问题是不足为奇的。
即使是现在从一个计算系统角度来看可能相当大的网络， 实际上比相对原始的脊椎动物如青蛙的神经系统更小。

由于有更快的CPU、通用GPU的到来（在\ref{sec:gpu_implementations}节中讨论）、更快的网络连接和更好的分布式计算的软件基础设施，模型的规模随着时间的推移不断增加是\gls{DL}历史中最重要的趋势之一。
普遍预计这种趋势将很好地持续到未来。

% -- 21 --

\subsection{与日俱增的精度、复杂度和对现实世界的冲击}
\label{sec:increasing_accuracy_complexity_and_real_world_impact}

<BAD>20世纪80年代以来，\gls{DL}一直在提高提供识别精度和预测的能力。
此外，\gls{DL}持续地被成功应用到越来越广泛的应用中去。

最早的深度模型被用来识别裁剪的很合适且非常小的图像中的单个对象\citep{Rumelhart86}。
自那时以来，\gls{NN}可以处理的图像尺寸逐渐增加。
现代对象识别网络能处理丰富的高分辨率照片，并且不要求在被识别的对象附近进行裁剪\citep{Krizhevsky-2012}。
类似地，最早网络只能识别两种对象（或在某些情况下，单一种类的对象的存在与否），而这些现代网络通常识别至少\NUMTEXT{1000}个不同类别的对象。
对象识别中最大的比赛是每年举行的ImageNet大型视觉识别挑战（ILSVRC）。
<BAD>\gls{DL}迅速崛起的一个戏剧性的时刻是卷积网络第一次大幅赢得这一挑战，将最高水准的前5错误率从\NUMTEXT{26.1\%}降到\NUMTEXT{15.3\%}\citep{Krizhevsky-2012}，这意味着该卷积网络产生针对每个图像的可能类别的分级列表，尽管\NUMTEXT{15.3\%}的测试样例的正确类别不会出现在此列表中的前5。

此后，深度卷积网络一直能在这些比赛中取胜，截至写书时，\gls{DL}的进步将这个比赛中的前5错误率降到\NUMTEXT{3.6％}， 如图\ref{fig:chap1_imagenet_color}所示。
\begin{figure}[!htb]
\ifOpenSource
\centerline{\includegraphics{figure.pdf}}
\else
\centerline{\includegraphics{Chapter1/figures/imagenet_color}}
\fi
\caption{TODO}
\label{fig:chap1_imagenet_color}
\end{figure}

% -- 23 --

\gls{DL}也对语音识别产生了巨大影响。
语音识别在20世纪90年代提高后，直到约2000年都停滞不前。
\gls{DL}的引入\citep{dahl2010phonerec,Deng-2010,Seide2011,Hinton-et-al-2012}导致语音识别错误率的陡然下降，有些错误率甚至降低了一半。 
我们将在\ref{sec:speech_recognition}节更详细地探讨这个历史。

深度网络在行人检测和图像分割中也有引人注目的成功\citep{sermanet-cvpr-13,Farabet-et-al-2013,couprie-iclr-13}，在交通标志分类上取得了超越人类的表现\citep{Ciresan-et-al-2012}。

在深度网络的规模和的精度有所提高的同时，它们可以解决的任务也日益复杂。
\citet{Goodfellow+et+al-ICLR2014a}表明，\gls{NN}可以学习输出描述图像的整个字符序列，而不是仅仅识别单个对象。
此前，人们普遍认为，这种学习需要对序列中的单个元素进行标注\citep{Gulcehre+Bengio-arxiv-2013}。
\gls{RNN}，如之前提到的\glssymbol{LSTM}序列模型，现在用于建模序列和其他序列之间的关系，而不是仅仅固定输入之间的关系。
这个序列到序列的学习似乎处于另一个应用演进的浪尖：机器翻译\citep{Sutskever-et-al-NIPS2014,Bahdanau-et-al-ICLR2015-small}。

% -- 24 --

日益复杂的趋势已将其推向逻辑结论，即神经图灵机\citep{Graves-et-al-arxiv2014}的引入，它能学习读取存储单元和向存储单元写入任意内容。
这样的\gls{NN}可以从期望行为的\gls{example}中学习简单的程序。
例如，从杂乱和排好序的\gls{example}中学习对一系列数进行排序。
这种自我编程技术正处于起步阶段，但原则上未来可以适用于几乎所有的任务。


\gls{DL}的另一个冠军成就是在\firstgls{RL}领域的扩展。
在\gls{RL}的背景下，一个自主体必须通过试错来学习执行任务，而无需人类操作者的任何指导。
DeepMind表明，基于\gls{DL}的\gls{RL}系统能够学会玩Atari视频游戏，并在多种任务中可与人类匹敌\citep{Mnih-et-al-2015}。
\gls{DL}也显著改善了机器人\gls{RL}的性能\citep{finn2015learning}。

许多\gls{DL}应用都是高利润的。现在\gls{DL}被许多顶级的技术公司使用，包括Google、Microsoft、Facebook、IBM、Baidu、Apple、Adobe、Netflix、NVIDIA和NEC。

\gls{DL}的进步也严重依赖于软件基础架构的进展。
软件库如Theano\citep{bergstra+al:2010-scipy,Bastien-2012}、PyLearn2\citep{pylearn2_arxiv_2013}、Torch\citep{Torch-2011}、DistBelief\citep{Dean-et-al-NIPS2012}、Caffe\citep{Jia13caffe}、MXNet\citep{chen2015mxnet}和TensorFlow\citep{tensorflow}都能支持重要的研究项目或商业产品。

\gls{DL}也为其他科学做出了贡献。
识别对象的现代卷积网络提供给神经科学家可以研究的视觉处理模型\citep{dicarlo-tutorial-2013}。
\gls{DL}也为处理海量数据、在科学领域作出的有效预测提供了非常有用的工具。
它已成功地用于预测分子如何相互作用，这能帮助制药公司设计新的药物\citep{Dahl-et-al-arxiv2014}、搜索的亚原子粒子\citep{baldi2014searching}和自动解析用于构建人脑三维图的显微镜图像\citep{knowlesdeep}。
我们期待\gls{DL}未来出现在越来越多的科学领域。

% -- 25 --

总之，\gls{DL}是\gls{ML}的一种方法， 过去几十年的发展中，它深深地吸收了我们关于人脑、统计学与应用数学的知识。
近年来，\gls{DL}的普及性和实用性有了极大的发展，这在很大程度上得益于更强大的计算机、更大的数据集和能够训练更深网络的技术。
未来几年充满了进一步提高\gls{DL}并将它带到新领域挑战和机遇。

% -- 26 --
